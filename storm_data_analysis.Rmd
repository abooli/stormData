---
title: "Storm Data Analysis"
author: "April Zhao"
date: "1/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
## **Synopsis**
This html file is a documentation of the data analysis project exploring the US National Oceanic and Atmospheric Administration's storm database. We will be primarily answering two questions:

1. Across the US, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?
2. Across the United States, which Types of events have the greatest economic consequences?

The documentation has the following parts: a **data processing** part that handles the retrieval and formatting of the dataset; a **results** section that explores and solves the two questions, and a **discussions** section that discusses the validity of the conclusions from the results section.


## **Data Processing**
(Loading necessary packages)
```{r load, echo = FALSE}
library(dplyr)
library(ggplot2)
library(lubridate)
```
To get the data ready to be analyzed, we first need to download the data to the directory and then preprpocess the data so it is in a format ready for us to use.

#### **Downloading the Data**
```{r download,cache=TRUE}
## The url is where the storm dataset is located and their filenames.
url <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
fileName <- "./data/storm.csv.bz2"


## Creates a data directory where we could download the data in.
if (!dir.exists("data")) {
  dir.create("data")
}

## Download the zipped dataset to the directory
if (!file.exists(fileName)) {
  download.file(url, fileName)
}

## Read in the dataframe
storm <- read.csv(fileName)
```

Now, since we have the data already loaded, we could take a look at what the data frame looks like.
```{r peek}
names(storm)
dim(storm)
```

Since the objective of this data analysis is to find the types of events (as indicated in the `EVTYPE` variable) are most harmful with respect to population health and economic consequences, it would be helpful for us to look at what event types we have, and how many each has:
```{r table}
## Counts the occurrences of each event type
type <- table(storm$EVTYPE)

## A peak at how many unique event types there are
length(type)

## A peak at the count of the first 10 event types
head(type, 10)
```

Now, since we have too many event types, it would be a good idea to keep the event types that occur the most frequently in our analysis. For example, it would be better if we keep the event types such as "BLIZZARD" and "AVALANCHE" while discarding the ones like "?" and "APACHE COUNTY" since they don't offer much insight to the data analysis. In this analysis, I will set a threshold of 150 in count, and we will be only using the event types that has more than 150 occurrences.

```{r subset}
## Reset type from a table to a data frame
typeDf <- cbind(names(type), type)

## Subset the ones with occurrences greater than 150.
type <- subset(typeDf, type > 150)

## Look at how many items we are left with now
dim(type)
head(type)
```

As shown above, we have narrowed the types of events to 60 events. Now, we will use this `type` data frame to subset the original `storm` data frame with the events that match the ones on this list.
```{r clean, cache = TRUE}
storm <- subset(storm, EVTYPE %in% type[,1])
dim(storm)
```

Notice how we are able to remove 5877 rows of data with irrelevant types that we won't be using in this analysis.



## **Results**
Now, with a clean data set, we could start with the analysis of the data frame to answer the following two questions:

1. Across the US, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?
2. Across the United States, which Types of events have the greatest economic consequences?

#### **Question 1**

Question 1 deals with the types of events that are the most harmful with respect to population health. The term "population health" refers to the fatality and injury brought by this storm. Therefore, we will be referring to the columns `FATALITIES` and `INJURIES` to answer this question.

First, calculate the average fatality and injury caused by each storm:
```{r average}
## Group by event type
groupedStorm <- group_by(storm, EVTYPE)

# Calculate the average fatality and injury caused by each event and then rearranging the data frame in descending order of the count
fatality <- 
  summarise(groupedStorm, fatalities = mean(FATALITIES, na.rm = TRUE)) %>%
  arrange(desc(fatalities), by_group = FALSE)

injury <- 
  summarise(groupedStorm, injuries = mean(INJURIES, na.rm = TRUE))  %>%
  arrange(desc(injuries), by_group = FALSE)

head(fatality)
head(injury)
```

Here is a plot showing the correlation between injuries and fatalities count:
```{r correlation}
totalHarm <- full_join(fatality, injury, by = "EVTYPE")
g <- ggplot(totalHarm, aes(injuries, fatalities))
g + geom_point()
```

From the graph above, we could see that there might be some weak correlation between injuries and fatalities. However, we still want to find the TOP 5 most dangerous event using a smart way. 

One tricky part to answering this question is to collapse the two variables, fatality and injury, into one variable. This requires us to put a value mark for the injury and fatality. In this analysis, we will use a 2:1 ratio for fatality and injury when we calculate a weighted sum of total harm, and we will find the TOP 5 most dangerous event.
```{r total}
double_Fatality <- fatality
double_Fatality$fatalities <- 2* fatality$fatalities

totalHarm <- full_join(double_Fatality, injury, by = "EVTYPE") %>%
  mutate(harm = fatalities + injuries) %>%
  arrange(desc(harm))

head(totalHarm, 6)
```

From the results above, we have found the TOP 5 most dangerous events, they are:

- **EXCESSIVE HEAT** with a total harmful level of 6.16
- **HEAT** with a total harmful level of 5.18
- **RIP CURRENTS** with a total harmful level of 2.15
- **TORNADO** with a total harmful level of 1.69
- **AVALANCHE** with a total harmful level of 1.60


2. Across the United States, which Types of events have the greatest economic consequences?

#### **Question 2**
While question 1 investigates the most harmful events for population health, question 2 dives into the types of events that cause the greatest economic consequences. We will be using the columns `PROPDMG` and `CROPDMG` to answer this question.


Unlike fatality vs injury, property and crop damage doesn't have to be weighted. Instead, they just have to be summed together. Therefore, we will create the following table summing the average total damage caused by each event:
```{r dmg}
## Find the average of property damage and crop damage grouped by the event type.
prop <- summarise(groupedStorm, propertyDamage = mean(PROPDMG, na.rm = TRUE))

crop <- summarise(groupedStorm, cropDamage = mean(CROPDMG, na.rm = TRUE))

## Join the two data frames into one and arrange them from the most damage to the least
totalDmg <- full_join(prop, crop, by = "EVTYPE") %>%
  mutate(totalDmg = propertyDamage + cropDamage) %>%
  arrange(desc(totalDmg))
  
head(totalDmg,5)
```

From the previous table, we could already tell the 5 most damaging events economic-wise. They are:

- **HURRICANE** with a total damage of 119.85 (units unknown)
- **RIVER FLOOD** with a total damage of 100.26 (units unknown)
- **TROPICAL STORM** with a total damage of 78.83 (units unknown)
- **STORM SURGE** with a total damage of 74.32 (units unknown)
- **URBAN FLOOD** with a total damage of 57.09 (units unknown)

## **Further Discussion**

However, I would still like to investigate more about this dataset. Unlike fatality and injury, the property value of things may or may not be influenced by inflation, especially because this dataset includes data from a very large period of time, over 60 years. Therefore, I will construct a plot showing the year vs property damage trend below, which would help us to gain insight on whether inflation is adjusted.
```{r year}
## Create function to take a substring of the poorly formatted date column
date_only <- function(char) {
  if (nchar(char) == 16) {
    substr(char, 1,8)
  } else if (nchar(char) == 17) {
    substr(char,1,9)
  } else {
    substr(char,1,10)
  }
}

## Change the column to be date objects
storm$BGN_DATE <- date_only(storm$BGN_DATE)
storm$BGN_DATE <- mdy(storm$BGN_DATE)

## Remove the missing values from the data frame
cleanStorm <- 
  storm %>%
  filter(!is.na(BGN_DATE))

## Add a new column showing the total damage
cleanStorm$totalDmg <- cleanStorm$PROPDMG + cleanStorm$CROPDMG

## Graph the begin date vs damage
h <- ggplot(cleanStorm, aes(BGN_DATE, totalDmg))
h + geom_line()
```

From the time-series plot above, we have found that there is significant increase in damage value across the years. Therefore, we have evidence that inflation is not adjusted. This undermines our analysis of the result based on different event types across the 50 year span.



